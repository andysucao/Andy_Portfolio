<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Question Answering on Andy Cao</title>
    <link>https://andysucao.github.io/Andy_Portfolio/tags/question-answering/</link>
    <description>Recent content in Question Answering on Andy Cao</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 20 Aug 2023 06:06:00 -0400</lastBuildDate>
    <atom:link href="https://andysucao.github.io/Andy_Portfolio/tags/question-answering/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Project 7: Extractive QA with a Fine-Tuned BERT</title>
      <link>https://andysucao.github.io/Andy_Portfolio/post/project-7/</link>
      <pubDate>Sun, 20 Aug 2023 06:06:00 -0400</pubDate>
      <guid>https://andysucao.github.io/Andy_Portfolio/post/project-7/</guid>
      <description>1. Overview In this project, we will build a Bidirectional Encoder Representations from Transformers (BERT) based model for a different Natural Language Processing task &amp;ndash; Question Answering. The model will be fine-tuned on the Conversational Question Answering Challenge (CoQA) dataset from Stanford University.&#xA;The Python Notebook containing the complete model development process and the data used in this project can be found at Google Drive.&#xA;2. Question Answering (QA) Question Answering, particularly Extraction-based Question Answering, is another type of Natural Language Processing task.</description>
    </item>
  </channel>
</rss>
