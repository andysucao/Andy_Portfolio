<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Project 5: Using Autoencoder for Anomaly Detection and searching similar images | Andy Cao</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.116.1">
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="https://andysucao.github.io/Andy_Portfolio/dist/css/app.1cb140d8ba31d5b2f1114537dd04802a.css" rel="stylesheet">
    

    

    
      
    

    
    
    <meta property="og:title" content="Project 5: Using Autoencoder for Anomaly Detection and searching similar images" />
<meta property="og:description" content="Autoencoder based tool for Anomaly Detection and searching similar images" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://andysucao.github.io/Andy_Portfolio/post/project-5/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2023-08-20T06:04:00-04:00" />
<meta property="article:modified_time" content="2023-08-20T06:04:00-04:00" />
<meta itemprop="name" content="Project 5: Using Autoencoder for Anomaly Detection and searching similar images">
<meta itemprop="description" content="Autoencoder based tool for Anomaly Detection and searching similar images"><meta itemprop="datePublished" content="2023-08-20T06:04:00-04:00" />
<meta itemprop="dateModified" content="2023-08-20T06:04:00-04:00" />
<meta itemprop="wordCount" content="1803">
<meta itemprop="keywords" content="Computer Vision,Anomaly Detection,Unsupervised Learning," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Project 5: Using Autoencoder for Anomaly Detection and searching similar images"/>
<meta name="twitter:description" content="Autoencoder based tool for Anomaly Detection and searching similar images"/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  
  <header class="cover bg-top" style="background-image: url('https://andysucao.github.io/Andy_Portfolio/images/projects-5-1d.jpg');">
    <div class="pb3-m pb6-l bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://andysucao.github.io/Andy_Portfolio/" class="f3 fw2 hover-white no-underline white-90 dib">
      Andy Cao
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://andysucao.github.io/Andy_Portfolio/about/" title="About page">
              About
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://andysucao.github.io/Andy_Portfolio/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://andysucao.github.io/Andy_Portfolio/post/" title="Projects page">
              Projects
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://andysucao.github.io/Andy_Portfolio/skills/" title="Useful skills page">
              Useful skills
            </a>
          </li>
          
        </ul>
      
      







<a href="https://www.linkedin.com/in/cao/" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://github.com/andysucao" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>







    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Project 5: Using Autoencoder for Anomaly Detection and searching similar images</h1>
          
            <h2 class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
              Autoencoder based tool for Anomaly Detection and searching similar images
            </h2>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        PROJECTS
      </aside>
      




  <div id="sharing" class="mt3">

    
    <a href="https://www.facebook.com/sharer.php?u=https://andysucao.github.io/Andy_Portfolio/post/project-5/" class="facebook no-underline" aria-label="share on Facebook">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

    </a>

    
    
    <a href="https://twitter.com/share?url=https://andysucao.github.io/Andy_Portfolio/post/project-5/&amp;text=Project%205:%20Using%20Autoencoder%20for%20Anomaly%20Detection%20and%20searching%20similar%20images" class="twitter no-underline" aria-label="share on Twitter">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

    </a>

    
    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://andysucao.github.io/Andy_Portfolio/post/project-5/&amp;title=Project%205:%20Using%20Autoencoder%20for%20Anomaly%20Detection%20and%20searching%20similar%20images" class="linkedin no-underline" aria-label="share on LinkedIn">
      <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

    </a>
  </div>


      <h1 class="f1 athelas mt3 mb1">Project 5: Using Autoencoder for Anomaly Detection and searching similar images</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2023-08-20T06:04:00-04:00">August 20, 2023</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><h2 id="1-overview">1. Overview</h2>
<p>Anomaly detection is a technique used for identifying rare items, events or observations, which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behavior <a href="https://en.wikipedia.org/wiki/Anomaly_detection">wikipedia</a>.</p>
<p>Anomaly detection has very wide applications, such as Fraud detection in credit card transactions <a href="https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud">ref</a>, Network Intrusion detection <a href="https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html">ref</a>, and Cancer cell detection <a href="https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data">ref</a>.</p>
<p>A wide spectrum of techniques have been proposed for anomaly detection, some of the popular methods are: Density-based techniques (e.g., k-nearest neighbor), Cluster analysis-based techniques, Ensemble techniques, Long Short-Term Memory (LSTM) neural networks, as well as Autoencoders.</p>
<p><a href="https://andysucao.github.io/Andy_Portfolio/images/projects-5-2.png"><img src="https://andysucao.github.io/Andy_Portfolio/images/projects-5-2.png" alt=""> </a></p>
<p>In this project, we will build an Autoencoder based anomaly detector, and we will use it to (1) determine whether the given image is similar to the training data (2) find images that are similar to the given image.</p>
<p>The Python Notebook containing the complete model development process and the data used in this project can be found at <a href="https://drive.google.com/drive/folders/1x3gRbwBYsXoe9P2Jd3egVRczFLwLva96?usp=sharing">Google Drive</a>.</p>
<p>           
           </p>
<h2 id="2-model-development">2. Model development</h2>
<h3 id="21-dataset">2.1. Dataset</h3>
<p>The dataset used in this project is the CIFAR-10 dataset from <a href="https://www.cs.toronto.edu/~kriz/cifar.html">University of Toronto</a>. This dataset consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The 10 classes are: plane, car, bird, cat, deer, dog, frog, horse, ship, and truck.</p>
<p>In this project, the training data contains 5000 images of dog from CIFAR-10 training images; the validation data contains 500 images of dog from CIFAR-10 test images; and the test data contains 500 images of dog and 500 images of car from CIFAR-10 test images. Sample images are shown below.</p>
<p><a href="https://andysucao.github.io/Andy_Portfolio/images/projects-5-3.png"><img src="https://andysucao.github.io/Andy_Portfolio/images/projects-5-3.png" alt=""> </a></p>
<p>           </p>
<h3 id="22-methodology">2.2. Methodology</h3>
<p>Generally, the anomaly detector will be trained on a set of training data first. Then for a new input x, the anomaly detector will detect if x is similar to the training data. If answer is yes, then the new input x is considered normal. On the other hand, if the new input x is considered different from training data, then it will be considered anomaly (also known as outlier, novelty, or exceptions).</p>
<p>Specifically in our project, we will train an Autoencoder with small reconstruction error first. The definition and code realization of reconstruction error are shown below, followed by a figure demonstrating the effects of shifting and masking on reconstruction error.</p>
<p><a href="https://andysucao.github.io/Andy_Portfolio/images/projects-5-6c.png"><img src="https://andysucao.github.io/Andy_Portfolio/images/projects-5-6c.png" alt=""> </a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compare_imgs</span>(img1, img2, title_prefix<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Calculate MSE loss between both images</span>
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>mse_loss(img1, img2, reduction<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sum&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Plot images for visual comparison</span>
</span></span><span style="display:flex;"><span>    grid <span style="color:#f92672">=</span> torchvision<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>make_grid(torch<span style="color:#f92672">.</span>stack([img1, img2], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>), nrow<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, normalize<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, range<span style="color:#f92672">=</span>(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>    grid <span style="color:#f92672">=</span> grid<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>title_prefix<span style="color:#e6db74">}</span><span style="color:#e6db74"> Loss: </span><span style="color:#e6db74">{</span>loss<span style="color:#f92672">.</span>item()<span style="color:#e6db74">:</span><span style="color:#e6db74">4.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>imshow(grid)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#39;off&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">2</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Load example image</span>
</span></span><span style="display:flex;"><span>    img <span style="color:#f92672">=</span> X_train[<span style="color:#ae81ff">32</span> <span style="color:#f92672">+</span> <span style="color:#ae81ff">128</span><span style="color:#f92672">*</span> i]
</span></span><span style="display:flex;"><span>    img <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>from_numpy(img)<span style="color:#f92672">.</span>float()
</span></span><span style="display:flex;"><span>    img_mean <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>mean(dim<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>], keepdims<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Shift image by one pixel</span>
</span></span><span style="display:flex;"><span>    SHIFT <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    img_shifted <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>roll(img, shifts<span style="color:#f92672">=</span>SHIFT, dims<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    img_shifted <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>roll(img_shifted, shifts<span style="color:#f92672">=</span>SHIFT, dims<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    img_shifted[:,:<span style="color:#ae81ff">1</span>,:] <span style="color:#f92672">=</span> img_mean
</span></span><span style="display:flex;"><span>    img_shifted[:,:,:<span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> img_mean
</span></span><span style="display:flex;"><span>    compare_imgs(img, img_shifted, <span style="color:#e6db74">&#34;Shifted -&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Set half of the image to zero</span>
</span></span><span style="display:flex;"><span>    img_masked <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>clone()
</span></span><span style="display:flex;"><span>    img_masked[:,:img_masked<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">//</span><span style="color:#ae81ff">2</span>,:] <span style="color:#f92672">=</span> img_mean
</span></span><span style="display:flex;"><span>    compare_imgs(img, img_masked, <span style="color:#e6db74">&#34;Masked -&#34;</span>)
</span></span></code></pre></div><p><a href="https://andysucao.github.io/Andy_Portfolio/images/projects-5-6b.png"><img src="https://andysucao.github.io/Andy_Portfolio/images/projects-5-6b.png" alt=""> </a></p>
<p>Then during the inference stage, we will use the reconstruction error as the anomaly score, as an image from an unseen distribution should have higher reconstruction error, and evaluate the performance of each model by its AUC (Area Under the Receiver operating characteristic (ROC) Curve) score.</p>
<p><a href="https://andysucao.github.io/Andy_Portfolio/images/projects-5-6d.png"><img src="https://andysucao.github.io/Andy_Portfolio/images/projects-5-6d.png" alt=""> </a></p>
<p>           </p>
<h3 id="23-structure-of-autoencoder-model">2.3. Structure of Autoencoder model</h3>
<p>A series of 12 autoencoder models are studied in this project. All of them share identical Encoder and Decoder, except the size of low dimensional bottleneck (also known as Latent, Embedding, Representation, and Code) Vector connecting them. The code block below shows the realization of an Autoencoder:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Autoencoder models - Encoder</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Encoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, num_input_channels : int, base_channel_size : int, latent_dim : int, act_fn : object <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>GELU):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        c_hid <span style="color:#f92672">=</span> base_channel_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>net <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(num_input_channels, c_hid, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>), <span style="color:#75715e"># 32x32 =&gt; 16x16</span>
</span></span><span style="display:flex;"><span>            act_fn(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(c_hid, c_hid, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            act_fn(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(c_hid, <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>c_hid, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>), <span style="color:#75715e"># 16x16 =&gt; 8x8</span>
</span></span><span style="display:flex;"><span>            act_fn(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>c_hid, <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>c_hid, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            act_fn(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>c_hid, <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>c_hid, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>), <span style="color:#75715e"># 8x8 =&gt; 4x4</span>
</span></span><span style="display:flex;"><span>            act_fn(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Flatten(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span><span style="color:#ae81ff">16</span><span style="color:#f92672">*</span>c_hid, latent_dim))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>net(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Autoencoder models - Decoder</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Decoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, num_input_channels : int, base_channel_size : int, latent_dim : int, act_fn : object <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>GELU):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        c_hid <span style="color:#f92672">=</span> base_channel_size
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(latent_dim, <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span><span style="color:#ae81ff">16</span><span style="color:#f92672">*</span>c_hid),
</span></span><span style="display:flex;"><span>            act_fn())
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>net <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ConvTranspose2d(<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>c_hid, <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>c_hid, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, output_padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>), <span style="color:#75715e"># 4x4 =&gt; 8x8</span>
</span></span><span style="display:flex;"><span>            act_fn(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>c_hid, <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>c_hid, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            act_fn(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ConvTranspose2d(<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>c_hid, c_hid, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, output_padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>), <span style="color:#75715e"># 8x8 =&gt; 16x16</span>
</span></span><span style="display:flex;"><span>            act_fn(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(c_hid, c_hid, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            act_fn(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ConvTranspose2d(c_hid, num_input_channels, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, output_padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>), <span style="color:#75715e"># 16x16 =&gt; 32x32</span>
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Tanh()) <span style="color:#75715e"># The input images is scaled between -1 and 1, hence the output has to be bounded as well</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>linear(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>reshape(x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>net(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Autoencoder</span>(pl<span style="color:#f92672">.</span>LightningModule):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self,base_channel_size: int,latent_dim: int,encoder_class : object <span style="color:#f92672">=</span> Encoder,decoder_class : object <span style="color:#f92672">=</span> Decoder,num_input_channels: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>,width: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">32</span>,height: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">32</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Saving hyperparameters of autoencoder</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>save_hyperparameters()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Creating encoder and decoder</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>encoder <span style="color:#f92672">=</span> encoder_class(num_input_channels, base_channel_size, latent_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>decoder <span style="color:#f92672">=</span> decoder_class(num_input_channels, base_channel_size, latent_dim)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Example input array needed for visualizing the graph of the network</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>example_input_array <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">2</span>, num_input_channels, width, height)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        z <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encoder(x)
</span></span><span style="display:flex;"><span>        x_hat <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>decoder(z)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x_hat
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_get_reconstruction_loss</span>(self, batch):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> batch
</span></span><span style="display:flex;"><span>        x_hat <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>forward(x)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>mse_loss(x, x_hat, reduction<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;none&#34;</span>)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> loss<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>])<span style="color:#f92672">.</span>mean(dim<span style="color:#f92672">=</span>[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> loss
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">configure_optimizers</span>(self):
</span></span><span style="display:flex;"><span>        optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>Adam(self<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-3</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Using a scheduler is optional but can be helpful.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># The scheduler reduces the LR if the validation performance hasn&#39;t improved for the last N epochs</span>
</span></span><span style="display:flex;"><span>        scheduler <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>lr_scheduler<span style="color:#f92672">.</span>ReduceLROnPlateau(optimizer, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;min&#39;</span>, factor<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, patience<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>, min_lr<span style="color:#f92672">=</span><span style="color:#ae81ff">5e-5</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#34;optimizer&#34;</span>: optimizer, <span style="color:#e6db74">&#34;lr_scheduler&#34;</span>: scheduler, <span style="color:#e6db74">&#34;monitor&#34;</span>: <span style="color:#e6db74">&#34;val_loss&#34;</span>}
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">training_step</span>(self, batch, batch_idx):
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_get_reconstruction_loss(batch)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>log(<span style="color:#e6db74">&#39;train_loss&#39;</span>, loss)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> loss
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">validation_step</span>(self, batch, batch_idx):
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_get_reconstruction_loss(batch)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>log(<span style="color:#e6db74">&#39;val_loss&#39;</span>, loss)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">test_step</span>(self, batch, batch_idx):
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_get_reconstruction_loss(batch)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>log(<span style="color:#e6db74">&#39;test_loss&#39;</span>, loss)
</span></span></code></pre></div><p>The figure below shows the summary of two autoencoders with latent dimension equals to 16 and 256. As can be seen from these figures, the only difference among various autoencoders is the size of latent vector. Also it is noticed that in order to accommodate the change in latent vector&rsquo;s size, the last layer of Encoder and first layer of Decoder are modified too. As a consequence, the number of total parameters of each autoencoder is different too.</p>
<p><a href="https://andysucao.github.io/Andy_Portfolio/images/projects-5-4.png"><img src="https://andysucao.github.io/Andy_Portfolio/images/projects-5-4.png" alt=""> </a></p>
<p><a href="https://andysucao.github.io/Andy_Portfolio/images/projects-5-5.png"><img src="https://andysucao.github.io/Andy_Portfolio/images/projects-5-5.png" alt=""> </a></p>
<p>           
           </p>
<h2 id="3-results-and-discussion">3. Results and discussion</h2>
<p>           </p>
<h3 id="31-reconstruction-error">3.1. Reconstruction error</h3>
<p>The figure below shows the result of reconstruction error as a function of latent vector dimension. From this figure, we can clearly observe the trend that, in general, as latent vector dimension increases, the reconstruction error will decrease significantly. This result is in good alignment with our expectation, because as the size of latent vector increases, less dimension reduction happened and more information is embedded in the latent vector. As a consequence, when Decoder tries to reconstruct the image using this larger latent vector, the reconstruction error will be lower.</p>
<p><a href="https://andysucao.github.io/Andy_Portfolio/images/projects-5-7.png"><img src="https://andysucao.github.io/Andy_Portfolio/images/projects-5-7.png" alt=""> </a></p>
<p>These pictures below shows the comparison of all 12 autoencoders with different latent vector. Two images from normal class (i.e., dog) and another two images from anomalous class (i.e., car) are employed. By taking a close look at these pictures, we can confirm that the quality of reconstructed images are indeed improved as we increase the size of the latent vector.</p>
<p><a href="https://andysucao.github.io/Andy_Portfolio/images/projects-5-8b.png"><img src="https://andysucao.github.io/Andy_Portfolio/images/projects-5-8b.png" alt=""> </a></p>
<p>           </p>
<h3 id="32-area-under-the-roc-curve-auc">3.2. Area under the ROC curve (AUC)</h3>
<p>The figure below shows the summary of AUC scores of each model with different latent vector dimension, and the ROC curve of each model is shown in the next picture. From these figures, it is found that the highest AUC of 0.8494 occurred when latent vector dimension equals 256.</p>
<p>When the dimension of latent vector is very small (e.g., 2, 4, or 8), the AUC value is quite low. This is because as the latent vector is very small, very little information is embedded in the latent vector, both the normal image and anomalous image cannot be reconstructed well by the Decoder. As a consequence, both of them have large reconstruction errors, and the AUC score becomes quite low.</p>
<p>On the other hand, when the dimension of the latent vector is very high (1024, 2048, or 4096), the dimension of latent vector is so large and so much information is embedded in the latent vector that, both the normal image and anomalous image can be reconstructed very well. As a consequence, both of them have small reconstruction errors, and the AUC score becomes lower again. This trend can be observed in the figure above as well.</p>
<p><a href="https://andysucao.github.io/Andy_Portfolio/images/projects-5-9.png"><img src="https://andysucao.github.io/Andy_Portfolio/images/projects-5-9.png" alt=""> </a></p>
<p><a href="https://andysucao.github.io/Andy_Portfolio/images/projects-5-10.png"><img src="https://andysucao.github.io/Andy_Portfolio/images/projects-5-10.png" alt=""> </a></p>
<p>The figure below shows examples of original and reconstructed images of dog (normal) and car (anomalous) from Autoencoder with latent vector dimension equals 256. The next image shows the corresponding distribution of reconstruction error for normal and anomalous (aka out of distribution) images. From both images, we can see that the reconstructed images of dog indeed have better quality compared with those of car.</p>
<p><a href="https://andysucao.github.io/Andy_Portfolio/images/projects-5-11.png"><img src="https://andysucao.github.io/Andy_Portfolio/images/projects-5-11.png" alt=""> </a></p>
<p><a href="https://andysucao.github.io/Andy_Portfolio/images/projects-5-12.png"><img src="https://andysucao.github.io/Andy_Portfolio/images/projects-5-12.png" alt=""> </a></p>
<p>           </p>
<h3 id="33-out-of-distribution-images">3.3. Out-of-distribution images</h3>
<p>In this section, we will try to use Autoencoder to reconstruct images that are truly anomalous (i.e., out-of-distribution). The images below shows the comparison of original images and reconstructed images for (1) random images, (2) single color channel, (3) checkerboard pattern, and (4) color progression, with latent vector dimension set to 256. From these results, we can see that for these out-of-distribution images, even though very simple, their reconstructed images are of very low quality. This is great results for Autoencoder when used as an anomaly detector.</p>
<p><a href="https://andysucao.github.io/Andy_Portfolio/images/projects-5-13.png"><img src="https://andysucao.github.io/Andy_Portfolio/images/projects-5-13.png" alt=""> </a></p>
<p><a href="https://andysucao.github.io/Andy_Portfolio/images/projects-5-14.png"><img src="https://andysucao.github.io/Andy_Portfolio/images/projects-5-14.png" alt=""> </a></p>
<p>           </p>
<h3 id="34-searching-similar-images">3.4. Searching similar images</h3>
<p>Another application of Autoencoder is to find similar images of a given image. The figure below shows some very interesting results, and its code realization is shown in the following code block. As can be seen from these results, the Autoencoder we built in this project can successfully find images that are similar to a given image.</p>
<p><a href="https://andysucao.github.io/Andy_Portfolio/images/projects-5-15.png"><img src="https://andysucao.github.io/Andy_Portfolio/images/projects-5-15.png" alt=""> </a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Finding visually similar images</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> model_dict[<span style="color:#ae81ff">256</span>][<span style="color:#e6db74">&#34;model&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">embed_imgs</span>(model, data_loader):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Encode all images in the data_loader using model, and return both images and encodings</span>
</span></span><span style="display:flex;"><span>    img_list, embed_list <span style="color:#f92672">=</span> [], []
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> imgs <span style="color:#f92672">in</span> tqdm(data_loader, desc<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Encoding images&#34;</span>, leave<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>            z <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>encoder(imgs<span style="color:#f92672">.</span>to(model<span style="color:#f92672">.</span>device))
</span></span><span style="display:flex;"><span>        img_list<span style="color:#f92672">.</span>append(imgs)
</span></span><span style="display:flex;"><span>        embed_list<span style="color:#f92672">.</span>append(z)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> (torch<span style="color:#f92672">.</span>cat(img_list, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>), torch<span style="color:#f92672">.</span>cat(embed_list, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_img_embeds <span style="color:#f92672">=</span> embed_imgs(model, train_loader)
</span></span><span style="display:flex;"><span>test_img_embeds <span style="color:#f92672">=</span> embed_imgs(model, test_loader)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">find_similar_images</span>(query_img, query_z, key_embeds, K<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Find closest K images. We use the euclidean distance here but other like cosine distance can also be used.</span>
</span></span><span style="display:flex;"><span>    dist <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cdist(query_z[<span style="color:#66d9ef">None</span>,:], key_embeds[<span style="color:#ae81ff">1</span>], p<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    dist <span style="color:#f92672">=</span> dist<span style="color:#f92672">.</span>squeeze(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    dist, indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sort(dist)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Plot K closest images</span>
</span></span><span style="display:flex;"><span>    imgs_to_display <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([query_img[<span style="color:#66d9ef">None</span>], key_embeds[<span style="color:#ae81ff">0</span>][indices[:K]]], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    grid <span style="color:#f92672">=</span> torchvision<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>make_grid(imgs_to_display, nrow<span style="color:#f92672">=</span>K<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>, normalize<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, range<span style="color:#f92672">=</span>(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>    grid <span style="color:#f92672">=</span> grid<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>,<span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>imshow(grid)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#39;off&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot the closest images for the first N test images as example</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">8</span>):
</span></span><span style="display:flex;"><span>    find_similar_images(test_img_embeds[<span style="color:#ae81ff">0</span>][i], test_img_embeds[<span style="color:#ae81ff">1</span>][i], key_embeds<span style="color:#f92672">=</span>train_img_embeds)
</span></span></code></pre></div><p>           
           </p>
<h2 id="4-conclusions">4. Conclusions</h2>
<p>In this project, I have developed a Autoencoder based model for anomaly detection that can successfully distinguish images of dog and car from CIFAR-10 dataset, with AUC equals to 0.8494. Furthermore, the Autoencoder model is also used to search for similar images of a given image, and very good results are obtained as well.</p>
<ul class="pa0">
  
   <li class="list">
     <a href="https://andysucao.github.io/Andy_Portfolio/tags/computer-vision" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Computer Vision</a>
   </li>
  
   <li class="list">
     <a href="https://andysucao.github.io/Andy_Portfolio/tags/anomaly-detection" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Anomaly Detection</a>
   </li>
  
   <li class="list">
     <a href="https://andysucao.github.io/Andy_Portfolio/tags/unsupervised-learning" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">Unsupervised Learning</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="https://andysucao.github.io/Andy_Portfolio/post/project-4/">Project 4: Image Classification and Explainable Artificial Intelligence</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://andysucao.github.io/Andy_Portfolio/" >
    &copy;  Andy Cao 2023 
  </a>
    <div>







<a href="https://www.linkedin.com/in/cao/" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://github.com/andysucao" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>






</div>
  </div>
</footer>

    

  <script src="https://andysucao.github.io/Andy_Portfolio/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
